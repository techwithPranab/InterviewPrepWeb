Order,Question,Answer,Category,Tags,Code Example,References
1,"What is Node.js and what are its key features?","Node.js is a JavaScript runtime built on Chrome's V8 engine that executes JavaScript code outside the browser. Key features include: non-blocking I/O, event-driven architecture, single-threaded with event loop, NPM ecosystem, cross-platform compatibility, and high scalability for I/O intensive operations.",Core Concepts,"nodejs, basics, runtime, v8","const http = require('http');\n\nconst server = http.createServer((req, res) => {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end('Hello World from Node.js!');\n});\n\nserver.listen(3000, () => {\n  console.log('Server running on port 3000');\n});",https://nodejs.org/en/docs/
2,"Explain the Node.js Event Loop and its phases in detail.","The Event Loop is Node.js's mechanism for handling asynchronous operations. It has 6 phases: Timers (setTimeout/setInterval), Pending Callbacks (I/O callbacks), Idle/Prepare (internal), Poll (retrieve new I/O events), Check (setImmediate), and Close Callbacks (socket closures). Microtasks (Promises, process.nextTick) run between phases.",Core Concepts,"event-loop, async, concurrency, performance","console.log('Start');\n\nsetTimeout(() => console.log('setTimeout'), 0);\nsetImmediate(() => console.log('setImmediate'));\nPromise.resolve().then(() => console.log('Promise'));\nprocess.nextTick(() => console.log('nextTick'));\n\nconsole.log('End');\n\n// Output: Start, End, nextTick, Promise, setTimeout, setImmediate",https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/
3,"What are Streams in Node.js and what types exist?","Streams are collections of data that might not be available all at once and don't have to fit in memory. Types: Readable (fs.createReadStream), Writable (fs.createWriteStream), Duplex (net.Socket - both read/write), Transform (zlib.createGzip - modify data). Streams are EventEmitters and handle large data efficiently using backpressure.",Core Concepts,"streams, memory-efficiency, data-processing","const fs = require('fs');\nconst zlib = require('zlib');\n\n// Efficient file compression using streams\nconst readStream = fs.createReadStream('large-file.txt');\nconst writeStream = fs.createWriteStream('large-file.txt.gz');\nconst gzip = zlib.createGzip();\n\nreadStream\n  .pipe(gzip)\n  .pipe(writeStream)\n  .on('finish', () => console.log('Compression complete'));",https://nodejs.org/en/docs/guides/backpressuring-in-streams/
4,"What is the difference between process.nextTick() and setImmediate()?","process.nextTick() executes callbacks in the same phase of the event loop, before moving to the next phase (microtask queue). setImmediate() executes in the Check phase of the next event loop iteration. nextTick has higher priority and can cause I/O starvation if used recursively. Use setImmediate for breaking up long operations.",Core Concepts,"event-loop, async, scheduling","// nextTick executes before I/O\nfs.readFile('file.txt', () => {\n  console.log('I/O callback');\n});\n\nprocess.nextTick(() => console.log('nextTick'));\nsetImmediate(() => console.log('setImmediate'));\n\n// Output order:\n// nextTick\n// setImmediate\n// I/O callback",https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/#process-nexttick-vs-setimmediate
5,"How does Node.js handle child processes and what methods are available?","Node.js provides the 'child_process' module with methods: spawn() (streams, long-running), exec() (buffers, shell commands), execFile() (no shell, faster), and fork() (IPC channel, Node.js processes). Use for CPU-intensive tasks, running system commands, or creating worker processes. Worker threads are now preferred for CPU-bound JavaScript tasks.",Concurrency,"child-process, workers, performance, clustering","const { spawn, fork } = require('child_process');\n\n// Spawn for streaming output\nconst ls = spawn('ls', ['-la']);\nls.stdout.on('data', (data) => console.log(data.toString()));\n\n// Fork for Node.js worker processes\nconst worker = fork('./worker.js');\nworker.send({ task: 'process-data', data: [1,2,3] });\nworker.on('message', (result) => console.log(result));",https://nodejs.org/en/docs/latest/api/child_process.html
6,"Explain middleware in Express.js and its types.","Middleware are functions with access to req, res, and next(). Types: Application-level (app.use), Router-level (router.use), Error-handling (4 params: err, req, res, next), Built-in (express.json, express.static), Third-party (cors, helmet). They execute sequentially and can modify request/response, end cycle, or call next().",Express.js,"express, middleware, request-handling, architecture","const express = require('express');\nconst app = express();\n\n// Application-level middleware\napp.use(express.json());\napp.use((req, res, next) => {\n  req.requestTime = Date.now();\n  next();\n});\n\n// Router-level middleware\nconst router = express.Router();\nrouter.use((req, res, next) => {\n  console.log('Router middleware');\n  next();\n});\n\n// Error-handling middleware (must be last)\napp.use((err, req, res, next) => {\n  res.status(err.status || 500).json({ error: err.message });\n});",https://expressjs.com/en/guide/using-middleware.html
7,"What are the differences between CommonJS and ES Modules in Node.js?","CommonJS (require/module.exports) loads synchronously, caches modules, and was Node.js default. ES Modules (import/export) load asynchronously, support tree-shaking, have static analysis, and are the JavaScript standard. Use .mjs extension or 'type: module' in package.json for ESM. Top-level await works only in ESM.",Module System,"esm, commonjs, modules, imports","// CommonJS\nconst fs = require('fs');\nmodule.exports = { myFunction };\n\n// ES Modules (package.json: type: module)\nimport fs from 'fs';\nimport { readFile } from 'fs/promises';\n\n// Top-level await (ESM only)\nconst data = await readFile('file.txt', 'utf8');\n\nexport { myFunction };\nexport default MyClass;",https://nodejs.org/en/docs/latest/api/esm.html
8,"How do you handle errors in Node.js asynchronously?","Error handling strategies: try-catch for async/await, .catch() for Promises, error-first callbacks, error event listeners for streams/EventEmitters, process.on('uncaughtException') as last resort. Use centralized error handling middleware in Express. Always handle Promise rejections to avoid deprecation warnings.",Error Handling,"error-handling, async, promises, best-practices","// Async/await with try-catch\nasync function fetchData() {\n  try {\n    const result = await someAsyncOperation();\n    return result;\n  } catch (error) {\n    console.error('Error:', error);\n    throw error; // Re-throw or handle\n  }\n}\n\n// Promise rejection handling\nprocess.on('unhandledRejection', (reason, promise) => {\n  console.error('Unhandled Rejection:', reason);\n  // Log, notify, graceful shutdown\n});\n\n// Error-first callback pattern\nfs.readFile('file.txt', (err, data) => {\n  if (err) return console.error(err);\n  console.log(data);\n});",https://nodejs.org/en/docs/latest/api/process.html#event-uncaughtexception
9,"What is clustering in Node.js and how does it improve performance?","Clustering allows creating child processes (workers) that share the same server port, utilizing multi-core systems. The master process distributes incoming connections across workers using round-robin. Improves performance for CPU-bound operations and provides fault tolerance. PM2 simplifies cluster management in production.",Scalability,"cluster, performance, scaling, multi-core","const cluster = require('cluster');\nconst http = require('http');\nconst numCPUs = require('os').cpus().length;\n\nif (cluster.isMaster) {\n  console.log(`Master ${process.pid} starting`);\n  // Fork workers\n  for (let i = 0; i < numCPUs; i++) {\n    cluster.fork();\n  }\n  cluster.on('exit', (worker) => {\n    console.log(`Worker ${worker.process.pid} died`);\n    cluster.fork(); // Replace dead worker\n  });\n} else {\n  // Workers share TCP connection\n  http.createServer((req, res) => {\n    res.end(`Worker ${process.pid} handled request`);\n  }).listen(8000);\n}",https://nodejs.org/en/docs/latest/api/cluster.html
10,"Explain Worker Threads and when to use them over child processes.","Worker Threads share memory and are lightweight, ideal for CPU-intensive JavaScript tasks. Use SharedArrayBuffer and Atomics for shared memory. Compared to child processes: faster startup, less memory, no IPC serialization overhead, but single V8 instance. Use for parallel JavaScript execution, image processing, encryption.",Concurrency,"worker-threads, performance, parallel-processing, cpu-intensive","const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');\n\nif (isMainThread) {\n  // Main thread\n  const worker = new Worker(__filename, {\n    workerData: { num: 5 }\n  });\n  worker.on('message', (result) => {\n    console.log('Result:', result);\n  });\n} else {\n  // Worker thread\n  function fibonacci(n) {\n    if (n <= 1) return n;\n    return fibonacci(n - 1) + fibonacci(n - 2);\n  }\n  const result = fibonacci(workerData.num);\n  parentPort.postMessage(result);\n}",https://nodejs.org/en/docs/latest/api/worker_threads.html
11,"What are Buffer objects and how are they used?","Buffers represent fixed-size chunks of memory allocated outside V8 heap. Used for binary data (files, network streams, images). Created via Buffer.from(), Buffer.alloc(), or Buffer.allocUnsafe(). Support various encodings (utf8, base64, hex). Essential for handling TCP streams and file system operations efficiently.",Core Concepts,"buffer, binary-data, memory, performance","// Creating buffers\nconst buf1 = Buffer.from('Hello', 'utf8');\nconst buf2 = Buffer.alloc(10); // Initialized with zeros\nconst buf3 = Buffer.allocUnsafe(10); // Faster but uninitialized\n\n// Working with buffers\nconsole.log(buf1.toString()); // 'Hello'\nconsole.log(buf1.toString('hex')); // Hex representation\n\n// Binary operations\nconst buf = Buffer.from([0x62, 0x75, 0x66, 0x66, 0x65, 0x72]);\nconsole.log(buf.toString()); // 'buffer'\n\n// Concatenation\nconst combined = Buffer.concat([buf1, buf2]);",https://nodejs.org/en/docs/latest/api/buffer.html
12,"How do you implement rate limiting in Node.js APIs?","Rate limiting prevents abuse by restricting request frequency. Implement using: express-rate-limit middleware, Redis for distributed systems, token bucket or sliding window algorithms. Configure per IP, user, or endpoint. Include headers: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset. Consider burst allowances and different tiers.",Security,"rate-limiting, security, api-protection, redis","const rateLimit = require('express-rate-limit');\nconst RedisStore = require('rate-limit-redis');\nconst redis = require('redis');\n\nconst client = redis.createClient();\n\n// Basic rate limiting\nconst limiter = rateLimit({\n  store: new RedisStore({ client }),\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // Limit each IP to 100 requests per windowMs\n  message: 'Too many requests, please try again later',\n  standardHeaders: true,\n  legacyHeaders: false,\n});\n\n// Apply to specific routes\napp.use('/api/', limiter);\n\n// Different limits for different endpoints\nconst strictLimiter = rateLimit({ windowMs: 60000, max: 5 });\napp.post('/api/login', strictLimiter, loginHandler);",https://www.npmjs.com/package/express-rate-limit
13,"What are the best practices for securing Node.js applications?","Security best practices: Use helmet.js for HTTP headers, validate/sanitize inputs, implement authentication (JWT, OAuth), use HTTPS, enable CORS properly, keep dependencies updated (npm audit), prevent SQL injection (parameterized queries), use rate limiting, implement CSRF protection, secure session management, hide error details in production, use environment variables for secrets, implement logging and monitoring.",Security,"security, helmet, authentication, best-practices","const helmet = require('helmet');\nconst express = require('express');\nconst rateLimit = require('express-rate-limit');\n\nconst app = express();\n\n// Security headers\napp.use(helmet());\n\n// CORS configuration\nconst cors = require('cors');\napp.use(cors({\n  origin: process.env.ALLOWED_ORIGINS.split(','),\n  credentials: true\n}));\n\n// Input validation\nconst { body, validationResult } = require('express-validator');\napp.post('/user', [\n  body('email').isEmail().normalizeEmail(),\n  body('password').isLength({ min: 8 })\n], (req, res) => {\n  const errors = validationResult(req);\n  if (!errors.isEmpty()) {\n    return res.status(400).json({ errors: errors.array() });\n  }\n});\n\n// Environment variables\nrequire('dotenv').config();",https://cheatsheetseries.owasp.org/cheatsheets/Nodejs_Security_Cheat_Sheet.html
14,"Explain JWT authentication and how to implement it in Node.js.","JWT (JSON Web Tokens) consists of header, payload, and signature. Used for stateless authentication. Workflow: user logs in → server generates JWT → client stores token → client sends token in Authorization header → server verifies signature. Use refresh tokens for long sessions, short expiry for access tokens, secure storage (httpOnly cookies), and blacklisting for logout.",Authentication,"jwt, authentication, security, tokens","const jwt = require('jsonwebtoken');\nconst bcrypt = require('bcrypt');\n\n// Login and generate JWT\napp.post('/login', async (req, res) => {\n  const user = await User.findOne({ email: req.body.email });\n  if (!user) return res.status(401).json({ error: 'Invalid credentials' });\n  const validPassword = await bcrypt.compare(req.body.password, user.password);\n  if (!validPassword) return res.status(401).json({ error: 'Invalid credentials' });\n  const accessToken = jwt.sign({ userId: user._id, role: user.role }, process.env.JWT_SECRET, { expiresIn: '15m' });\n  const refreshToken = jwt.sign({ userId: user._id }, process.env.REFRESH_SECRET, { expiresIn: '7d' });\n  res.json({ accessToken, refreshToken });\n});\n\n// Verify middleware\nconst verifyToken = (req, res, next) => {\n  const token = req.headers['authorization']?.split(' ')[1];\n  if (!token) return res.status(403).json({ error: 'No token provided' });\n  jwt.verify(token, process.env.JWT_SECRET, (err, decoded) => {\n    if (err) return res.status(401).json({ error: 'Invalid token' });\n    req.userId = decoded.userId;\n    next();\n  });\n};",https://jwt.io/introduction
15,"How do you handle database connections efficiently in Node.js?","Use connection pooling to reuse connections (MongoDB, PostgreSQL drivers support it). Implement: connection retry logic, graceful shutdown, connection monitoring, read replicas for scaling, connection limits, prepared statements, lazy connections, health checks. Use ORMs like Mongoose, Sequelize, or Prisma for abstraction and schema management.",Database,"database, connection-pooling, mongodb, postgresql, performance","// MongoDB with Mongoose\nconst mongoose = require('mongoose');\nmongoose.connect(process.env.MONGO_URI, { maxPoolSize: 10, serverSelectionTimeoutMS: 5000, socketTimeoutMS: 45000 });\nconst db = mongoose.connection;\ndb.on('error', console.error);\ndb.once('open', () => console.log('Connected to MongoDB'));\n\n// PostgreSQL with pg\nconst { Pool } = require('pg');\nconst pool = new Pool({ host: process.env.DB_HOST, database: process.env.DB_NAME, user: process.env.DB_USER, password: process.env.DB_PASSWORD, max: 20, idleTimeoutMillis: 30000, connectionTimeoutMillis: 2000 });\n\nconst getUser = async (id) => {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT * FROM users WHERE id = $1', [id]);\n    return result.rows[0];\n  } finally {\n    client.release();\n  }\n};\n\n// Graceful shutdown\nprocess.on('SIGTERM', async () => { await pool.end(); await mongoose.connection.close(); });",https://mongoosejs.com/docs/connections.html
16,"What is the purpose of package-lock.json and how does it differ from package.json?","package.json defines dependencies with version ranges (^1.0.0 allows minor updates). package-lock.json locks exact versions and dependency tree for reproducible builds. It tracks nested dependencies, integrity hashes (SHA), and resolved URLs. Commit package-lock.json to git. npm ci uses lock file for faster, reliable installs in CI/CD. Yarn uses yarn.lock similarly.",Dependency Management,"npm, package-management, versioning, ci-cd","// package.json (semantic versioning)\n{\n  dependencies: {\n    express: ^4.18.0,  // Allows 4.18.x, 4.19.x, not 5.x\n    lodash: ~4.17.21   // Allows 4.17.x only\n  }\n}\n\n// package-lock.json ensures exact versions\n{\n  express: {\n    version: 4.18.2,\n    resolved: https://registry.npmjs.org/express/-/express-4.18.2.tgz,\n    integrity: sha512-5/...,\n    requires: {\n      body-parser: 1.20.1\n    }\n  }\n}\n\n// CI/CD best practice\nnpm ci  // Uses lock file, faster and reproducible",https://docs.npmjs.com/cli/v9/configuring-npm/package-lock-json
17,"Explain WebSockets and how to implement real-time communication in Node.js.","WebSockets provide full-duplex communication over a single TCP connection. Unlike HTTP, connection stays open for bidirectional data flow. Use cases: chat apps, live notifications, collaborative tools, gaming. Implement using socket.io (with fallbacks), ws library, or native WebSocket API. Handle reconnection, room management, authentication, and scaling with Redis adapter.",Real-time,"websockets, socket.io, real-time, bidirectional","const express = require('express');\nconst http = require('http');\nconst socketIO = require('socket.io');\n\nconst app = express();\nconst server = http.createServer(app);\nconst io = socketIO(server, { cors: { origin: '*' } });\n\n// Authentication middleware\nio.use((socket, next) => {\n  const token = socket.handshake.auth.token;\n  if (isValidToken(token)) {\n    socket.userId = getUserIdFromToken(token);\n    next();\n  } else {\n    next(new Error('Authentication error'));\n  }\n});\n\n// Connection handling\nio.on('connection', (socket) => {\n  console.log('User connected:', socket.userId);\n  socket.join(`user:${socket.userId}`);\n  socket.on('message', (data) => {\n    io.to(data.roomId).emit('message', { userId: socket.userId, text: data.text, timestamp: Date.now() });\n  });\n  socket.on('disconnect', () => {\n    console.log('User disconnected:', socket.userId);\n  });\n});\n\nserver.listen(3000);",https://socket.io/docs/v4/
18,"How do you implement logging and monitoring in production Node.js applications?","Use structured logging libraries (Winston, Pino, Bunyan) with log levels (error, warn, info, debug). Log to stdout/stderr and aggregate with ELK stack, Datadog, or CloudWatch. Include request IDs, timestamps, context. Monitor with APM tools (New Relic, Datadog APM), track metrics (CPU, memory, event loop lag), implement health checks, use distributed tracing (OpenTelemetry), and error tracking (Sentry).",DevOps,"logging, monitoring, winston, observability, production","const winston = require('winston');\n\n// Configure Winston logger\nconst logger = winston.createLogger({\n  level: process.env.LOG_LEVEL || 'info',\n  format: winston.format.combine(winston.format.timestamp(), winston.format.errors({ stack: true }), winston.format.json()),\n  defaultMeta: { service: 'api-service' },\n  transports: [\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\n    new winston.transports.File({ filename: 'combined.log' }),\n  ],\n});\n\nif (process.env.NODE_ENV !== 'production') {\n  logger.add(new winston.transports.Console({ format: winston.format.simple() }));\n}\n\n// Request logging middleware\napp.use((req, res, next) => {\n  req.id = require('uuid').v4();\n  logger.info('Request received', { requestId: req.id, method: req.method, url: req.url, ip: req.ip });\n  next();\n});\n\n// Health check endpoint\napp.get('/health', (req, res) => {\n  const health = { uptime: process.uptime(), message: 'OK', timestamp: Date.now(), memory: process.memoryUsage() };\n  res.json(health);\n});",https://github.com/winstonjs/winston
19,"What are the differences between npm, yarn, and pnpm?","npm is the default Node.js package manager with npm registry. Yarn (v1) added lock files, workspaces, faster installs. Yarn v2+ (Berry) uses Plug'n'Play, zero-installs. pnpm uses content-addressable storage, hard links for deduplication (saves disk space), fast installs, strict node_modules structure. pnpm prevents phantom dependencies. Choose based on: speed (pnpm/yarn), disk space (pnpm), compatibility (npm), features (yarn v2+).",Dependency Management,"npm, yarn, pnpm, package-managers, comparison","// npm\nnpm install\nnpm install express --save\nnpm ci  // Clean install from lock file\n\n// Yarn\nyarn install\nyarn add express\nyarn install --frozen-lockfile  // CI/CD\n\n// pnpm (most efficient)\npnpm install\npnpm add express\npnpm install --frozen-lockfile\n\n// Disk space comparison (100 projects):\n// npm: ~500MB per project = 50GB\n// Yarn: Similar to npm\n// pnpm: Shared store + hard links = ~5GB total\n\n// Workspace (monorepo) support\n// package.json\n{ workspaces: [packages/*] }",https://pnpm.io/motivation
20,"Explain how to implement graceful shutdown in Node.js applications.","Graceful shutdown ensures in-flight requests complete before terminating. Handle SIGTERM/SIGINT signals, stop accepting new connections, wait for existing requests, close database connections, flush logs, exit with appropriate code. Use timeout for force kill. Essential for zero-downtime deployments, Kubernetes, and preventing data loss.",DevOps,"graceful-shutdown, production, kubernetes, deployment, reliability","const express = require('express');\nconst app = express();\nlet server;\nlet isShuttingDown = false;\nlet connections = new Set();\n\nserver = app.listen(3000, () => { console.log('Server running on port 3000'); });\n\n// Track connections\nserver.on('connection', (conn) => {\n  connections.add(conn);\n  conn.on('close', () => connections.delete(conn));\n});\n\n// Graceful shutdown function\nasync function shutdown(signal) {\n  if (isShuttingDown) return;\n  isShuttingDown = true;\n  console.log(`${signal} received, starting graceful shutdown`);\n  server.close(() => { console.log('HTTP server closed'); });\n  await mongoose.connection.close();\n  await redisClient.quit();\n  setTimeout(() => { console.error('Forced shutdown after timeout'); process.exit(1); }, 30000);\n  for (const conn of connections) { conn.destroy(); }\n  console.log('Graceful shutdown completed');\n  process.exit(0);\n}\n\n// Listen for termination signals\nprocess.on('SIGTERM', () => shutdown('SIGTERM'));\nprocess.on('SIGINT', () => shutdown('SIGINT'));\nprocess.on('uncaughtException', (err) => { console.error('Uncaught Exception:', err); shutdown('uncaughtException'); });",https://expressjs.com/en/advanced/healthcheck-graceful-shutdown.html
21,"What is the difference between process.env and environment variables in Node.js?","process.env is a global object containing user environment variables. It's populated at startup and provides access to system environment variables and those defined in .env files (via dotenv). Environment variables are key-value pairs set at OS level or runtime, used for configuration without hardcoding values. They're strings by default and should be parsed for other types.",Configuration,"environment-variables, configuration, process, security","// .env file\nNODE_ENV=production\nPORT=3000\nDB_URL=mongodb://localhost:27017/myapp\nAPI_KEY=secret123\n\n// app.js\nrequire('dotenv').config();\n\nconsole.log(process.env.NODE_ENV); // 'production'\nconsole.log(process.env.PORT); // '3000' (string)\nconsole.log(process.env.DB_URL);\n\n// Type conversion needed\nconst port = parseInt(process.env.PORT, 10) || 3000;\nconst isProduction = process.env.NODE_ENV === 'production';\nconst features = process.env.ENABLED_FEATURES?.split(',') || [];\n\n// Validation\nif (!process.env.DB_URL) {\n  throw new Error('DB_URL environment variable is required');\n}",https://nodejs.org/en/docs/latest/api/process.html#processenv
22,"How do you implement caching strategies in Node.js applications?","Caching strategies include: in-memory (node-cache, memory-cache), Redis for distributed caching, HTTP caching (ETags, Cache-Control headers), CDN caching, and database query caching. Implement cache-aside, write-through, or write-behind patterns. Consider TTL (Time To Live), cache invalidation, cache warming, and memory limits for performance optimization.",Performance,"caching, redis, performance, memory, optimization","const NodeCache = require('node-cache');\nconst redis = require('redis');\n\n// In-memory caching\nconst cache = new NodeCache({ stdTTL: 600 }); // 10 minutes TTL\n\n// Redis caching\nconst redisClient = redis.createClient();\n\n// Cache-aside pattern\nasync function getUser(userId) {\n  // Check cache first\n  const cacheKey = `user:${userId}`;\n  let user = cache.get(cacheKey);\n  \n  if (!user) {\n    // Cache miss - fetch from database\n    user = await User.findById(userId);\n    if (user) {\n      cache.set(cacheKey, user, 300); // Cache for 5 minutes\n    }\n  }\n  return user;\n}\n\n// HTTP caching with ETags\napp.get('/api/users/:id', async (req, res) => {\n  const user = await getUser(req.params.id);\n  const etag = generateETag(user);\n  \n  if (req.headers['if-none-match'] === etag) {\n    return res.status(304).end(); // Not Modified\n  }\n  \n  res.set('ETag', etag);\n  res.set('Cache-Control', 'public, max-age=300');\n  res.json(user);\n});",https://www.npmjs.com/package/node-cache
23,"What are the best practices for handling file uploads in Node.js?","File upload best practices: validate file types/size, use middleware like multer, store files outside web root, implement virus scanning, use unique filenames, set upload limits, validate file content (not just extension), use cloud storage (AWS S3, Google Cloud), implement progress tracking, clean up temporary files, and secure file serving with proper headers.",File Handling,"file-uploads, multer, security, storage, validation","const multer = require('multer');\nconst path = require('path');\nconst crypto = require('crypto');\n\n// Configure multer\nconst storage = multer.diskStorage({\n  destination: (req, file, cb) => {\n    cb(null, 'uploads/');\n  },\n  filename: (req, file, cb) => {\n    const uniqueName = crypto.randomUUID() + path.extname(file.originalname);\n    cb(null, uniqueName);\n  }\n});\n\nconst upload = multer({\n  storage,\n  limits: {\n    fileSize: 5 * 1024 * 1024, // 5MB limit\n    files: 5 // Max 5 files\n  },\n  fileFilter: (req, file, cb) => {\n    const allowedTypes = ['image/jpeg', 'image/png', 'application/pdf'];\n    if (allowedTypes.includes(file.mimetype)) {\n      cb(null, true);\n    } else {\n      cb(new Error('Invalid file type'), false);\n    }\n  }\n});\n\n// Upload endpoint\napp.post('/upload', upload.array('files', 5), (req, res) => {\n  if (!req.files || req.files.length === 0) {\n    return res.status(400).json({ error: 'No files uploaded' });\n  }\n  \n  const fileInfo = req.files.map(file => ({\n    filename: file.filename,\n    originalname: file.originalname,\n    size: file.size,\n    mimetype: file.mimetype\n  }));\n  \n  res.json({ message: 'Files uploaded successfully', files: fileInfo });\n});",https://www.npmjs.com/package/multer
24,"Explain the concept of backpressure in Node.js streams and how to handle it.","Backpressure occurs when data is written to a stream faster than it can be consumed, causing memory buildup. Node.js streams handle this automatically by returning false from write() when internal buffer is full. Handle by: pausing readable streams, implementing drain events, using pipeline() for automatic backpressure management, monitoring highWaterMark, and implementing flow control.",Streams,"backpressure, streams, memory-management, flow-control, performance","const fs = require('fs');\nconst { pipeline } = require('stream');\nconst { Transform } = require('stream');\n\n// Manual backpressure handling\nfunction processLargeFile(inputPath, outputPath) {\n  const readable = fs.createReadStream(inputPath);\n  const writable = fs.createWriteStream(outputPath);\n  \n  readable.on('data', (chunk) => {\n    const ok = writable.write(chunk);\n    if (!ok) {\n      // Backpressure detected - pause reading\n      readable.pause();\n      writable.once('drain', () => {\n        readable.resume();\n      });\n    }\n  });\n}\n\n// Automatic backpressure with pipeline\nconst transform = new Transform({\n  transform(chunk, encoding, callback) {\n    // Process chunk (e.g., uppercase text)\n    const processed = chunk.toString().toUpperCase();\n    callback(null, processed);\n  }\n});\n\npipeline(\n  fs.createReadStream('input.txt'),\n  transform,\n  fs.createWriteStream('output.txt'),\n  (err) => {\n    if (err) console.error('Pipeline failed:', err);\n    else console.log('Pipeline succeeded');\n  }\n);",https://nodejs.org/en/docs/guides/backpressuring-in-streams/
25,"What is the purpose of the cluster module and how does it differ from worker threads?","Cluster module creates separate Node.js processes sharing the same port, each with its own V8 instance and memory. Worker threads run within the same process, sharing memory and suitable for CPU-intensive tasks. Clusters provide isolation and fault tolerance but have higher memory overhead. Use clusters for scaling I/O operations, worker threads for parallel JavaScript computation.",Concurrency,"cluster, worker-threads, scaling, processes, performance","// Cluster for I/O scaling\nconst cluster = require('cluster');\nconst express = require('express');\nconst numCPUs = require('os').cpus().length;\n\nif (cluster.isMaster) {\n  for (let i = 0; i < numCPUs; i++) {\n    cluster.fork();\n  }\n  cluster.on('exit', (worker) => {\n    console.log(`Worker ${worker.process.pid} died`);\n    cluster.fork();\n  });\n} else {\n  const app = express();\n  app.get('/', (req, res) => {\n    res.send(`Process ${process.pid} handled request`);\n  });\n  app.listen(3000);\n}\n\n// Worker threads for CPU tasks\nconst { Worker, isMainThread, parentPort } = require('worker_threads');\n\nif (isMainThread) {\n  const worker = new Worker(__filename);\n  worker.postMessage(1000000);\n  worker.on('message', (result) => {\n    console.log('Prime count:', result);\n  });\n} else {\n  parentPort.on('message', (n) => {\n    let count = 0;\n    for (let i = 2; i <= n; i++) {\n      let isPrime = true;\n      for (let j = 2; j * j <= i; j++) {\n        if (i % j === 0) { isPrime = false; break; }\n      }\n      if (isPrime) count++;\n    }\n    parentPort.postMessage(count);\n  });\n}",https://nodejs.org/en/docs/latest/api/cluster.html
26,"How do you implement database transactions in Node.js?","Database transactions ensure ACID properties for multiple operations. In MongoDB, use sessions with transactions (replica set required). In SQL databases (PostgreSQL, MySQL), use BEGIN/COMMIT/ROLLBACK. Implement try-catch blocks for error handling, rollback on failures, use connection pooling, and consider distributed transactions for microservices with tools like Saga pattern.",Database,"transactions, acid, mongodb, postgresql, data-consistency","// MongoDB transactions with Mongoose\nconst mongoose = require('mongoose');\n\nasync function transferMoney(fromUserId, toUserId, amount) {\n  const session = await mongoose.startSession();\n  \n  try {\n    await session.withTransaction(async () => {\n      // Debit from sender\n      const fromUser = await User.findById(fromUserId).session(session);\n      if (fromUser.balance < amount) {\n        throw new Error('Insufficient funds');\n      }\n      await User.findByIdAndUpdate(\n        fromUserId,\n        { $inc: { balance: -amount } },\n        { session }\n      );\n      \n      // Credit to receiver\n      await User.findByIdAndUpdate(\n        toUserId,\n        { $inc: { balance: amount } },\n        { session }\n      );\n      \n      // Log transaction\n      await Transaction.create([{\n        from: fromUserId,\n        to: toUserId,\n        amount,\n        timestamp: new Date()\n      }], { session });\n    });\n    \n    console.log('Transaction completed successfully');\n  } catch (error) {\n    console.error('Transaction failed:', error);\n    throw error;\n  } finally {\n    await session.endSession();\n  }\n}\n\n// PostgreSQL transactions with pg\nconst { Pool } = require('pg');\nconst pool = new Pool();\n\nasync function updateUserAndLog(userId, data) {\n  const client = await pool.connect();\n  \n  try {\n    await client.query('BEGIN');\n    \n    const userResult = await client.query(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3 RETURNING *',\n      [data.name, data.email, userId]\n    );\n    \n    await client.query(\n      'INSERT INTO audit_log (user_id, action, timestamp) VALUES ($1, $2, NOW())',\n      [userId, 'user_updated']\n    );\n    \n    await client.query('COMMIT');\n    return userResult.rows[0];\n  } catch (error) {\n    await client.query('ROLLBACK');\n    throw error;\n  } finally {\n    client.release();\n  }\n}",https://mongoosejs.com/docs/transactions.html
27,"What are webhooks and how do you implement them securely in Node.js?","Webhooks are HTTP callbacks triggered by events in external systems. Implementation involves: creating POST endpoints to receive payloads, verifying signatures (HMAC-SHA256), validating payload structure, implementing idempotency, handling retries, logging events, and ensuring endpoint security. Use for payment processing, CI/CD triggers, real-time integrations.",Integration,"webhooks, security, hmac, third-party, events","const express = require('express');\nconst crypto = require('crypto');\nconst app = express();\n\n// Middleware to capture raw body for signature verification\napp.use('/webhook', express.raw({ type: 'application/json' }));\n\n// Webhook signature verification\nfunction verifyWebhookSignature(payload, signature, secret) {\n  const expectedSignature = crypto\n    .createHmac('sha256', secret)\n    .update(payload)\n    .digest('hex');\n  \n  const providedSignature = signature.replace('sha256=', '');\n  \n  return crypto.timingSafeEqual(\n    Buffer.from(expectedSignature, 'hex'),\n    Buffer.from(providedSignature, 'hex')\n  );\n}\n\n// GitHub webhook example\napp.post('/webhook/github', (req, res) => {\n  const signature = req.headers['x-hub-signature-256'];\n  const event = req.headers['x-github-event'];\n  \n  if (!verifyWebhookSignature(req.body, signature, process.env.GITHUB_WEBHOOK_SECRET)) {\n    return res.status(401).json({ error: 'Invalid signature' });\n  }\n  \n  const payload = JSON.parse(req.body.toString());\n  \n  // Idempotency check\n  const deliveryId = req.headers['x-github-delivery'];\n  if (processedDeliveries.has(deliveryId)) {\n    return res.status(200).json({ message: 'Already processed' });\n  }\n  \n  // Process webhook based on event type\n  switch (event) {\n    case 'push':\n      handlePushEvent(payload);\n      break;\n    case 'pull_request':\n      handlePullRequestEvent(payload);\n      break;\n    default:\n      console.log(`Unhandled event: ${event}`);\n  }\n  \n  processedDeliveries.add(deliveryId);\n  res.status(200).json({ message: 'Webhook processed' });\n});",https://docs.github.com/en/developers/webhooks-and-events/webhooks/securing-your-webhooks
28,"How do you implement API versioning in Node.js REST APIs?","API versioning strategies: URL path versioning (/api/v1/users), header versioning (Accept: application/vnd.api+json;version=1), query parameter (?version=1), subdomain (v1.api.example.com). Implement middleware for version routing, maintain backward compatibility, use semantic versioning, document breaking changes, and plan deprecation strategies.",API Design,"api-versioning, rest, backward-compatibility, routing","const express = require('express');\nconst app = express();\n\n// URL Path Versioning\nconst v1Router = express.Router();\nconst v2Router = express.Router();\n\n// V1 API - Legacy format\nv1Router.get('/users/:id', async (req, res) => {\n  const user = await User.findById(req.params.id);\n  res.json({\n    id: user._id,\n    name: user.fullName,\n    email: user.email\n  });\n});\n\n// V2 API - New format with additional fields\nv2Router.get('/users/:id', async (req, res) => {\n  const user = await User.findById(req.params.id);\n  res.json({\n    id: user._id,\n    firstName: user.firstName,\n    lastName: user.lastName,\n    email: user.email,\n    profile: {\n      avatar: user.avatar,\n      bio: user.bio\n    },\n    metadata: {\n      createdAt: user.createdAt,\n      lastLogin: user.lastLogin\n    }\n  });\n});\n\napp.use('/api/v1', v1Router);\napp.use('/api/v2', v2Router);\n\n// Header-based versioning middleware\nconst versionMiddleware = (req, res, next) => {\n  const version = req.headers['api-version'] || 'v1';\n  req.apiVersion = version;\n  next();\n};\n\napp.use('/api', versionMiddleware);\napp.get('/api/users/:id', (req, res) => {\n  if (req.apiVersion === 'v2') {\n    // Use v2 logic\n  } else {\n    // Use v1 logic (default)\n  }\n});\n\n// Deprecation warnings\napp.use('/api/v1', (req, res, next) => {\n  res.set('Sunset', 'Wed, 01 Jan 2025 00:00:00 GMT');\n  res.set('Deprecation', 'true');\n  res.set('Link', '</api/v2>; rel=\"successor-version\"');\n  next();\n});",https://restfulapi.net/versioning/
29,"What are the security vulnerabilities specific to Node.js and how to prevent them?","Node.js specific vulnerabilities: prototype pollution (modify Object.prototype), dependency vulnerabilities (outdated packages), server-side template injection, regex denial of service (ReDoS), buffer overflow, path traversal, and unvalidated redirects. Prevention: input validation, dependency scanning (npm audit), secure coding practices, regular updates, OWASP guidelines.",Security,"security-vulnerabilities, prototype-pollution, redos, owasp","// Prototype Pollution Prevention\nconst _ = require('lodash');\n\n// Vulnerable code\nfunction merge(target, source) {\n  for (let key in source) {\n    if (typeof source[key] === 'object') {\n      if (!target[key]) target[key] = {};\n      merge(target[key], source[key]);\n    } else {\n      target[key] = source[key];\n    }\n  }\n}\n\n// Malicious payload\nconst malicious = JSON.parse('{\"__proto__\": {\"isAdmin\": true}}');\n// merge({}, malicious); // DON'T DO THIS\n\n// Safe implementation\nfunction safeMerge(target, source) {\n  for (let key in source) {\n    if (key === '__proto__' || key === 'constructor' || key === 'prototype') {\n      continue; // Skip dangerous keys\n    }\n    if (typeof source[key] === 'object' && source[key] !== null) {\n      if (!target[key] || typeof target[key] !== 'object') {\n        target[key] = {};\n      }\n      safeMerge(target[key], source[key]);\n    } else {\n      target[key] = source[key];\n    }\n  }\n}\n\n// ReDoS Prevention\nconst validator = require('validator');\n\n// Vulnerable regex\nconst vulnerableRegex = /^(a+)+$/;\n// Input like 'aaaaaaaaaaaaaaaaaaaX' causes exponential backtracking\n\n// Safe validation\napp.post('/api/validate', (req, res) => {\n  const { email, input } = req.body;\n  \n  // Use well-tested libraries\n  if (!validator.isEmail(email)) {\n    return res.status(400).json({ error: 'Invalid email' });\n  }\n  \n  // Timeout for regex operations\n  const safeRegexTest = (pattern, str, timeout = 100) => {\n    return new Promise((resolve) => {\n      const timer = setTimeout(() => resolve(false), timeout);\n      const result = pattern.test(str);\n      clearTimeout(timer);\n      resolve(result);\n    });\n  };\n});\n\n// Dependency vulnerability checking\n// Run: npm audit --audit-level moderate\n// Fix: npm audit fix",https://cheatsheetseries.owasp.org/cheatsheets/Nodejs_Security_Cheat_Sheet.html
30,"Explain microservices architecture patterns in Node.js and communication strategies.","Microservices patterns: API Gateway (single entry point), Service Discovery (dynamic service location), Circuit Breaker (fault tolerance), Event-driven architecture (async messaging), Database per service, Saga pattern (distributed transactions). Communication: HTTP/REST, gRPC, message queues (RabbitMQ, Apache Kafka), WebSockets. Use containers (Docker) and orchestration (Kubernetes).",Architecture,"microservices, api-gateway, circuit-breaker, event-driven, distributed","// API Gateway with Express\nconst express = require('express');\nconst httpProxy = require('http-proxy-middleware');\nconst CircuitBreaker = require('opossum');\n\nconst app = express();\n\n// Circuit breaker configuration\nconst options = {\n  timeout: 3000,\n  errorThresholdPercentage: 50,\n  resetTimeout: 30000\n};\n\nconst userServiceBreaker = new CircuitBreaker(callUserService, options);\nuserServiceBreaker.fallback(() => ({ error: 'User service unavailable' }));\n\nasync function callUserService(req) {\n  const response = await fetch(`http://user-service:3001${req.path}`);\n  return response.json();\n}\n\n// Service routing with circuit breaker\napp.use('/api/users', async (req, res, next) => {\n  try {\n    const result = await userServiceBreaker.fire(req);\n    res.json(result);\n  } catch (error) {\n    res.status(503).json({ error: 'Service temporarily unavailable' });\n  }\n});\n\n// Direct proxy for other services\napp.use('/api/orders', httpProxy({\n  target: 'http://order-service:3002',\n  changeOrigin: true,\n  pathRewrite: { '^/api/orders': '' }\n}));\n\n// Event-driven communication with message queue\nconst amqp = require('amqplib');\n\nclass EventBus {\n  async connect() {\n    this.connection = await amqp.connect('amqp://localhost');\n    this.channel = await this.connection.createChannel();\n  }\n  \n  async publish(exchange, routingKey, message) {\n    await this.channel.assertExchange(exchange, 'topic');\n    this.channel.publish(exchange, routingKey, Buffer.from(JSON.stringify(message)));\n  }\n  \n  async subscribe(exchange, queue, routingKey, callback) {\n    await this.channel.assertExchange(exchange, 'topic');\n    await this.channel.assertQueue(queue);\n    await this.channel.bindQueue(queue, exchange, routingKey);\n    this.channel.consume(queue, (msg) => {\n      const content = JSON.parse(msg.content.toString());\n      callback(content);\n      this.channel.ack(msg);\n    });\n  }\n}\n\n// Usage\nconst eventBus = new EventBus();\neventBus.publish('user.events', 'user.created', { userId: '123', email: 'user@example.com' });",https://microservices.io/patterns/microservices.html
